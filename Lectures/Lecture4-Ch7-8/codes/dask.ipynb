{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dask DataFrame (Parallelized Pandas)\n",
    "\n",
    "**Problem**: When datasets are too large to fit into memory (RAM), tools like Pandas or NumPy that operate entirely in memory become inefficient or even unusable.\n",
    "\n",
    "**Dask Solution**: Dask can break the data into smaller chunks and load/process data in an out-of-core fashion, meaning it loads only the parts it needs, processes them, and then moves on to the next chunk.\n",
    "\n",
    "**Example**: Handling a large CSV file that can't fit into memory using Dask DataFrame, which works similarly to a Pandas DataFrame but splits the data into partitions that are processed in parallel."
   ],
   "id": "c160b05221560880"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:03:25.047838Z",
     "start_time": "2024-10-20T01:03:10.694039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Create a large Pandas DataFrame\n",
    "df = pd.DataFrame({'A': range(1000000), 'B': range(1000000)})\n",
    "\n",
    "# Convert it to a Dask DataFrame with 4 partitions\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "# Perform an operation (like sum) on the Dask DataFrame\n",
    "result = ddf['A'].sum().compute()  # .compute() triggers the computation\n",
    "print(result)\n"
   ],
   "id": "610237ba70aa418e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499999500000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dask Array (Parallelized NumPy)\n",
    "\n",
    "**Problem**: Pythonâ€™s popular data science libraries like Pandas, NumPy, and Scikit-learn are single-threaded, meaning they run on a single CPU core, limiting performance when handling large tasks.\n",
    "\n",
    "**Dask Solution**: Dask extends these libraries by parallelizing operations across multiple CPU cores or even multiple machines. It supports the same APIs but distributes the computation across available resources.\n",
    "\n",
    "**Example**: Using Dask Array (similar to NumPy arrays) to perform large-scale computations in parallel.\n"
   ],
   "id": "81f0a5de184838c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:03:55.110422Z",
     "start_time": "2024-10-20T01:03:54.949243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Create a Dask Array with random values, split into chunks of size 1000x1000\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# Perform operations on the Dask Array\n",
    "result = x.mean().compute()  # .compute() triggers the parallel computation\n",
    "print(result)\n"
   ],
   "id": "92e6f17c542204d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999619067171989\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dask Delayed (Custom Task Parallelization)\n",
    "\n",
    "dask.delayed is a tool that allows you to parallelize any custom Python function or workflow.\n",
    "\n",
    "You build a task graph by wrapping functions with delayed, and then trigger execution when calling .compute()."
   ],
   "id": "885038184e760054"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:05:27.351864Z",
     "start_time": "2024-10-20T01:05:26.339174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dask import delayed\n",
    "import time\n",
    "\n",
    "# Custom function to simulate a time-consuming task\n",
    "def slow_add(x, y):\n",
    "    time.sleep(1)\n",
    "    return x + y\n",
    "\n",
    "# Use Dask delayed to parallelize the computation\n",
    "delayed_sum = delayed(slow_add)(5, 10)\n",
    "result = delayed_sum.compute()  # This runs the function in parallel\n",
    "print(result)"
   ],
   "id": "f260ba5a1155dc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scaling Machine Learning Workflows\n",
    "\n",
    "**Problem**: Scikit-learn and other machine learning libraries are limited to in-memory datasets and single-core computations. Training large models or performing hyperparameter optimization on big datasets can be very slow.\n",
    "\n",
    "**Dask Solutio**: Dask provides parallelized implementations of many machine learning algorithms (via Dask-ML), and it can also scale Scikit-learn workflows across multiple cores or machines.\n",
    "\n",
    "**Example**: Using Dask to parallelize Scikit-learn's GridSearchCV or training large models on large datasets.\n"
   ],
   "id": "275df42e0b015fbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a large dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20)\n",
    "\n",
    "# Use Dask to perform hyperparameter tuning\n",
    "model = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [50, 100, 200]}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X, y)  # Parallel hyperparameter search"
   ],
   "id": "380ba1b1cf45c116"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
